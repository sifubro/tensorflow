{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96f7996",
   "metadata": {},
   "source": [
    "### https://www.tensorflow.org/guide/autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e2694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea1ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b75ea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0e0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4408c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
       "array([[-0.06563916,  2.0368328 ],\n",
       "       [-0.2923617 ,  0.96828437],\n",
       "       [ 1.3369893 ,  0.26643306]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f797d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49f91d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[3.3601685, 4.772705 ]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x @ w + b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa63cf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.290732, 22.778715]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y**2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df822f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.034723"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y**2).numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "557e7e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=17.034723>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.reduce_mean(y**2)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f42636",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3046a18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 3.359375 ,  4.7734375],\n",
       "        [ 6.71875  ,  9.546875 ],\n",
       "        [10.078125 , 14.3203125]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.3601685, 4.772705 ], dtype=float32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloss_dw = tape.gradient(loss, w)\n",
    "dloss_db = tape.gradient(loss, b)\n",
    "\n",
    "dloss_dw, dloss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa14820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 3.359375 ,  4.7734375],\n",
       "        [ 6.71875  ,  9.546875 ],\n",
       "        [10.078125 , 14.3203125]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.3601685, 4.772705 ], dtype=float32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
    "\n",
    "dloss_dw, dloss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e8ff8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.3601685, 4.772705 ], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "def40819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "376cc22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4591c39",
   "metadata": {},
   "source": [
    "### If you have many sources a gradient for each is computed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df076b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7bdf5",
   "metadata": {},
   "source": [
    "### If you ask gradient on multiple targets or non-scalar the gradients for each are SUMMED and return a scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd16b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient(y0, x).numpy())\n",
    "print(tape.gradient(y1, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444b9fc",
   "metadata": {},
   "source": [
    "Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
    "\n",
    "- The gradient of the sum of the targets, or equivalently\n",
    "- The sum of the gradients of each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d209e4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ec65e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "#'Similarly, if the target(s) are not scalar the gradient of the sum is calculated:'\n",
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96e776",
   "metadata": {},
   "source": [
    "If you need a separate gradient for each item, refer to **Jacobians.** (https://www.tensorflow.org/guide/advanced_autodiff#jacobians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efc2ae",
   "metadata": {},
   "source": [
    "### Control flow\n",
    "\n",
    "https://www.tensorflow.org/guide/autodiff#control_flow\n",
    "\n",
    "Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, if and while statements).\n",
    "\n",
    "Here a different variable is used on each branch of an if. The gradient only connects to the variable that was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d4f2631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  if x > 0.0:\n",
    "    result = v0\n",
    "  else:\n",
    "    result = v1**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "881f663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "dx = tape.gradient(result, x)\n",
    "\n",
    "print(dx) # None + 1.0 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c2ef6",
   "metadata": {},
   "source": [
    "### Getting a gradient of None\n",
    "\n",
    "https://www.tensorflow.org/guide/autodiff#getting_a_gradient_of_none\n",
    "\n",
    "When a target is not connected to a source you will get a gradient of None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "156d41b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Here z is obviously not connected to x\n",
    "x = tf.Variable(2.)\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y * y\n",
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61647a4",
   "metadata": {},
   "source": [
    "### a) Replaced a variable with a tensor:\n",
    "\n",
    "One common error is to inadvertently replace a tf.Variable with a tf.Tensor, instead of using Variable.assign to update the tf.Variable\n",
    "\n",
    "### Variable + Tensor = Tensor !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cbf547f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.resource_variable_ops.ResourceVariable"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9389e4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x+1\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "980189e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "  with tf.GradientTape() as tape:\n",
    "    x = x+1 #x is a tensor now\n",
    "    y = x+5  \n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1aef2051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    x = x+1 #x is a tensor now\n",
    "    y = x**2  \n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6ea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2573ab7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y = x+5  \n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6713f9",
   "metadata": {},
   "source": [
    "### b) Did calculations outside of TensorFlow\n",
    "\n",
    "The tape can't record the gradient path if the calculation exits TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfe78b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x2 = x**2 # x2 is a tensor\n",
    "\n",
    "    # This step is calculated with NumPy\n",
    "    y = np.mean(x2, axis=0)\n",
    "\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac367264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.framework.ops.EagerTensor,\n",
       " tensorflow.python.ops.resource_variable_ops.ResourceVariable)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "x2 = x**2\n",
    "\n",
    "type(x2), type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2605061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.mean(x2, axis=0)\n",
    "\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8638e2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1482707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    #x2 = x**2 # x2 is a tensor\n",
    "\n",
    "    # This step is calculated with NumPy\n",
    "    y = np.mean(x, axis=0)\n",
    "\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e005fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.resource_variable_ops.ResourceVariable"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65a18a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.mean(x, axis=0)\n",
    "\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "429d8808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "353decb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x2 = x**2 # x2 is a tensor\n",
    "\n",
    "    # This step is calculated with NumPy\n",
    "    #y = np.mean(x, axis=0)\n",
    "\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y = tf.reduce_mean(x2, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c868e83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.ops.resource_variable_ops.ResourceVariable,\n",
       " tensorflow.python.framework.ops.EagerTensor,\n",
       " tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "x2 = x**2\n",
    "\n",
    "x3 = x + 1\n",
    "\n",
    "type(x), type(x2), type(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f39e68",
   "metadata": {},
   "source": [
    "### But 'x' remains a Variable! That's why dy_dx works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c524267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    x2 = x**2 # x2 is a tensor\n",
    "    x3 = x+1\n",
    "\n",
    "    # This step is calculated with NumPy\n",
    "    #y = np.mean(x, axis=0)\n",
    "\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y1 = tf.reduce_mean(x2, axis=0)\n",
    "    y2 = tf.reduce_mean(x3, axis=0)\n",
    "    \n",
    "print(tape.gradient(y1, x))\n",
    "print(tape.gradient(y2, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4183c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2. 4.]\n",
      " [6. 8.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    x2 = x**2 # x2 is a tensor\n",
    "    x3 = x+1\n",
    "\n",
    "    # This step is calculated with NumPy\n",
    "    #y = np.mean(x, axis=0)\n",
    "\n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y1 = x2+2\n",
    "    y2 = x3+3\n",
    "    \n",
    "print(tape.gradient(y1, x))\n",
    "print(tape.gradient(y2, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38189d",
   "metadata": {},
   "source": [
    "### But 'x' remains a Variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c3a90",
   "metadata": {},
   "source": [
    "### c) Took gradients through an integer or string\n",
    "\n",
    "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n",
    "\n",
    "Nobody expects strings to be differentiable, but it's easy to accidentally create an int constant or variable if you don't specify the dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "194bc6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f29630f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(20.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10.0)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bfc552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(20.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def725ed",
   "metadata": {},
   "source": [
    "### d) Took gradients through a stateful object\n",
    "\n",
    "## I DONT UNDERSTAND THIS!!!\n",
    "\n",
    "State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n",
    "\n",
    "A tf.Tensor is immutable. You can't change a tensor once it's created. It has a value, but no state. All the operations discussed so far are also stateless: the output of a tf.matmul only depends on its inputs.\n",
    "\n",
    "A tf.Variable has internal state—its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be81ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "tf.Tensor(12.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "x2 = tf.Variable(5.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # Update x1 = x1 + x0.\n",
    "    x1.assign_add(x0)\n",
    "    # The tape starts recording from x1.\n",
    "    y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "    x2 = x2 + 1\n",
    "    y2 = x2**2\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)\n",
    "print(tape.gradient(y, x1))\n",
    "print(tape.gradient(y2, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d68a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2798ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x2 = tf.Variable(5.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    x2 = x2 + 1\n",
    "    y2 = x2**2\n",
    "\n",
    "print(tape.gradient(y2, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec598c6",
   "metadata": {},
   "source": [
    "## WHY if I do another epoch then there is a problem???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11df4ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 BEFORE addition] x2=<class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\n",
      "[Epoch 0 AFTER addition] x2=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "y==<class 'tensorflow.python.framework.ops.EagerTensor'> at epoch 0\n",
      "tf.Tensor(12.0, shape=(), dtype=float32)\n",
      "---------------------------------------------------------------------------\n",
      "[Epoch 1 BEFORE addition] x2=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "[Epoch 1 AFTER addition] x2=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "y==<class 'tensorflow.python.framework.ops.EagerTensor'> at epoch 1\n",
      "None\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x2 = tf.Variable(5.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        print(f'[Epoch {epoch} BEFORE addition] x2={type(x2)}')\n",
    "        x2 = x2 + 1\n",
    "        print(f'[Epoch {epoch} AFTER addition] x2={type(x2)}')\n",
    "        y2 = x2**2\n",
    "        print(f'y=={type(y)} at epoch {epoch}')\n",
    "\n",
    "    print(tape.gradient(y2, x2))\n",
    "    print('-'*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f6367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "675ceaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0]\n",
      "type(x)=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "type(y)=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "type(z)=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(18.0, shape=(), dtype=float32)\n",
      "[Epoch 1]\n",
      "type(x)=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "type(y)=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "type(z)=<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(18.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        print(f'[Epoch {epoch}]')\n",
    "        tape.watch(x)\n",
    "        print(f'type(x)={type(x)}')\n",
    "        y = x * x\n",
    "        print(f'type(y)={type(y)}')\n",
    "        z = y * y\n",
    "        print(f'type(z)={type(z)}')\n",
    "\n",
    "    print(tape.gradient(z, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef203af",
   "metadata": {},
   "source": [
    "## MY EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5d657b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "  array([[0.61572266, 0.39233398],\n",
       "         [1.2314453 , 0.78466797],\n",
       "         [1.847168  , 1.177002  ]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.615712 , 0.3922553], dtype=float32)>],\n",
       " [<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "  array([[2.0927715, 0.       , 0.       , 1.994812 ],\n",
       "         [1.9531488, 0.       , 0.       , 1.8617249]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.8031466 , 0.        , 0.        , 0.76576364], dtype=float32)>],\n",
       " [<tf.Tensor: shape=(4, 6), dtype=float32, numpy=\n",
       "  array([[1.3563614 , 1.2142563 , 0.        , 0.        , 0.84202576,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.6641536 , 0.59457064, 0.        , 0.        , 0.41230488,\n",
       "          0.        ]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(6,), dtype=float32, numpy=\n",
       "  array([0.62460107, 0.5590073 , 0.        , 0.        , 0.38767642,\n",
       "         0.        ], dtype=float32)>]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = tf.keras.layers.Dense(2, activation='relu')\n",
    "layer2 = tf.keras.layers.Dense(4, activation='relu')\n",
    "layer3 = tf.keras.layers.Dense(6, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "def model(x):\n",
    "    # Forward pass\n",
    "    y = layer1(x)\n",
    "    z = layer2(y)\n",
    "    w = layer3(z)\n",
    "    \n",
    "    model.trainable_variables = [layer1.trainable_variables, \n",
    "                                 layer2.trainable_variables,\n",
    "                                 layer3.trainable_variables\n",
    "                                ]\n",
    "    \n",
    "    return w\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    w = model(x)\n",
    "    loss = tf.reduce_mean(w**2)\n",
    "\n",
    "print([var.name for var in tape.watched_variables()])\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c192104a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "  array([[-0.00325584,  0.03123474],\n",
       "         [-0.00651169,  0.06246948],\n",
       "         [-0.00976753,  0.09370422]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.00325643,  0.03123997], dtype=float32)>],\n",
       " [<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "  array([[0.        , 0.        , 0.24695492, 0.        ],\n",
       "         [0.        , 0.        , 0.36380506, 0.        ]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.        , 0.        , 0.08926678, 0.        ], dtype=float32)>],\n",
       " [<tf.Tensor: shape=(4, 6), dtype=float32, numpy=\n",
       "  array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.2539487 , 0.        , 0.06801331, 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        ]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(6,), dtype=float32, numpy=\n",
       "  array([0.        , 0.19171429, 0.        , 0.05135012, 0.        ,\n",
       "         0.        ], dtype=float32)>])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1 = tf.keras.layers.Dense(2, activation='relu')\n",
    "layer2 = tf.keras.layers.Dense(4, activation='relu')\n",
    "layer3 = tf.keras.layers.Dense(6, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # Forward pass\n",
    "    y = layer1(x)\n",
    "    z = layer2(y)\n",
    "    w = layer3(z)\n",
    "    loss = tf.reduce_mean(w**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad1 = tape.gradient(loss, layer1.trainable_variables)\n",
    "grad2 = tape.gradient(loss, layer2.trainable_variables)\n",
    "grad3 = tape.gradient(loss, layer3.trainable_variables)\n",
    "\n",
    "grad1, grad2, grad3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e46eb7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_18/kernel:0', 'dense_18/bias:0', 'dense_19/kernel:0', 'dense_19/bias:0', 'dense_20/kernel:0', 'dense_20/bias:0']\n"
     ]
    }
   ],
   "source": [
    "print([var.name for var in tape.watched_variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4659b",
   "metadata": {},
   "source": [
    "### If you watch tf.constant then everything works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "12f98c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0]\n",
      "8.0\n",
      "32.0\n",
      "262.0\n",
      "536.0\n",
      "17152.0\n",
      "[Epoch: 1]\n",
      "10.0\n",
      "50.0\n",
      "508.0\n",
      "1288.0\n",
      "64400.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f'[Epoch: {epoch}]')\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        #x = x + 1  # or here instead of the end of tape.gradient\n",
    "        tape.watch(x)\n",
    "        y = x * x + 2*x +1 \n",
    "        w = y * y + x**2 + 3\n",
    "        z = w * w \n",
    "        \n",
    "    # Use the tape to compute the gradient of z with respect to the\n",
    "    # intermediate value y.\n",
    "    # dz_dy = 2 * y and y = x ** 2 = 9\n",
    "    print(tape.gradient(y, x).numpy())\n",
    "    print(tape.gradient(w, y).numpy())\n",
    "    print(tape.gradient(w, x).numpy())\n",
    "    print(tape.gradient(z, w).numpy())\n",
    "    print(tape.gradient(z, y).numpy())\n",
    "    \n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318530f9",
   "metadata": {},
   "source": [
    "### For variables you must be careful when adding tensors to them\n",
    "\n",
    "### Variable + tensor = tensor hence gradient will not be watched!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6a39e323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0]\n",
      "6.0\n",
      "[Epoch: 1]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11764/2587804762.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# dy = 2x * dx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdy_dx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy_dx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# problem with second epoch is that x in a tensor now not a variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f'[Epoch: {epoch}]')\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x**2\n",
    "\n",
    "    # dy = 2x * dx\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(dy_dx.numpy())\n",
    "    \n",
    "    # problem with second epoch is that x in a tensor now not a variable\n",
    "    x = x + 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a142dc8",
   "metadata": {},
   "source": [
    "### But if you do tape.watch(x) manually then everything works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff50c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0]\n",
      "6.0\n",
      "[Epoch: 1]\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f'[Epoch: {epoch}]')\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = x**2\n",
    "\n",
    "    # dy = 2x * dx\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(dy_dx.numpy())\n",
    "    \n",
    "    x = x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a829f3",
   "metadata": {},
   "source": [
    "### Instead you could just do x.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "08876164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0]\n",
      "6.0\n",
      "[Epoch: 1]\n",
      "8.0\n",
      "[Epoch: 2]\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f'[Epoch: {epoch}]')\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x**2\n",
    "\n",
    "    # dy = 2x * dx\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print(dy_dx.numpy())\n",
    "    \n",
    "    #x = x + 1\n",
    "    x.assign_add(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fbd7a9",
   "metadata": {},
   "source": [
    "### Variable state prevents gradient from going back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fbe77488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # Update x1 = x1 + x0.\n",
    "#     print('Before assign')\n",
    "#     print([var.name for var in tape.watched_variables()])\n",
    "    x1.assign_add(x0)\n",
    "#     print('After assign')\n",
    "#     print([var.name for var in tape.watched_variables()])\n",
    "    # The tape starts recording from x1.\n",
    "    y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x1))\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c97186",
   "metadata": {},
   "source": [
    "### If you do x = x + 1 on a Variable it will result in error in the second iteration\n",
    "### But *WHY* it doesn't result in error in the first iteration?? Need to investigate that!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d1124faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0]\n",
      "type(x)=ResourceVariable\n",
      "type(y)=EagerTensor\n",
      "type(x)=EagerTensor\n",
      "gradient dy/dx= 8.0\n",
      "[Epoch: 1]\n",
      "type(x)=EagerTensor\n",
      "type(y)=EagerTensor\n",
      "type(x)=EagerTensor\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11764/3367570562.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'type(x)={type(x).__name__}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdy_dx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gradient dy/dx='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy_dx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f'[Epoch: {epoch}]')\n",
    "    with tf.GradientTape() as tape:\n",
    "        print(f'type(x)={type(x).__name__}')\n",
    "        x = x + 1\n",
    "        y = x**2\n",
    "        print(f'type(y)={type(y).__name__}')\n",
    "\n",
    "    print(f'type(x)={type(x).__name__}')\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print('gradient dy/dx=', dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "debb4459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0]\n",
      "type(x)=ResourceVariable\n",
      "type(y)=EagerTensor\n",
      "gradient dy/dx= 6.0\n",
      "After addition...\n",
      "type(x)=EagerTensor\n",
      "==============================\n",
      "[Epoch: 1]\n",
      "type(x)=EagerTensor\n",
      "type(y)=EagerTensor\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11764/3208419576.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# dy = 2x * dx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdy_dx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gradient dy/dx='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy_dx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# problem with second epoch is that x in a tensor now not a variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f'[Epoch: {epoch}]')\n",
    "    with tf.GradientTape() as tape:\n",
    "        print(f'type(x)={type(x).__name__}')\n",
    "        y = x**2\n",
    "        print(f'type(y)={type(y).__name__}')\n",
    "\n",
    "    # dy = 2x * dx\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print('gradient dy/dx=', dy_dx.numpy())\n",
    "    \n",
    "    # problem with second epoch is that x in a tensor now not a variable\n",
    "    x = x + 1  \n",
    "    print(f'After addition...')\n",
    "    print(f'type(x)={type(x).__name__}')\n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856ac9f",
   "metadata": {},
   "source": [
    "### However if you use a separate variable then that's fine. \n",
    "### I think it is because x remains a Variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b7d7231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0]\n",
      "type(x)=ResourceVariable\n",
      "type(y)=EagerTensor\n",
      "type(x)=ResourceVariable\n",
      "gradient dy/dx= 8.0\n",
      "gradient dy/dx= 8.0\n",
      "[Epoch: 1]\n",
      "type(x)=ResourceVariable\n",
      "type(y)=EagerTensor\n",
      "type(x)=ResourceVariable\n",
      "gradient dy/dx= 8.0\n",
      "gradient dy/dx= 8.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(f'[Epoch: {epoch}]')\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        print(f'type(x)={type(x).__name__}')\n",
    "        x1 = x + 1\n",
    "        y = x1**2\n",
    "        print(f'type(y)={type(y).__name__}')\n",
    "\n",
    "    print(f'type(x)={type(x).__name__}')\n",
    "    dy_dx = tape.gradient(y, x)\n",
    "    print('gradient dy/dx=', dy_dx.numpy())\n",
    "    dy_dx1 = tape.gradient(y, x1)\n",
    "    print('gradient dy/dx=', dy_dx1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3e402",
   "metadata": {},
   "source": [
    "### I think this doesn't produce an error because x0 remains a Variable.\n",
    "### But what about x1??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb562e0c",
   "metadata": {},
   "source": [
    "### If you change x1 as follows: x1 = x1 +1 then you get error on second iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "537bd69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "type(x0)=ResourceVariable\n",
      "type(x1)=EagerTensor\n",
      "type(x1)=EagerTensor\n",
      "type(y)=EagerTensor\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.598076, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "\n",
      "Epoch: 1\n",
      "type(x0)=ResourceVariable\n",
      "type(x1)=EagerTensor\n",
      "type(x1)=EagerTensor\n",
      "type(y)=EagerTensor\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print('')\n",
    "    print(f'Epoch: {epoch}')\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Update x1 = x1 + x0.\n",
    "        x2 = x1 + x0 + 1\n",
    "        \n",
    "        x1 = x1 + 1  ## If you change x1 then on second iteration will get an error\n",
    "\n",
    "        y = x1**1.5 + x2  # y = (x1 + x0)**2\n",
    "\n",
    "        print(f'type(x0)={type(x0).__name__}')\n",
    "        print(f'type(x1)={type(x1).__name__}')\n",
    "        print(f'type(x1)={type(x2).__name__}')\n",
    "        print(f'type(y)={type(y).__name__}')\n",
    "\n",
    "    # This doesn't work.\n",
    "    print(tape.gradient(y, x0))   #x0 remains a variable thats fine\n",
    "    print(tape.gradient(y, x1))   #x1 is a tensor though. This should get error\n",
    "    print(tape.gradient(y, x2))   #x1 is a tensor though. This should get error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f281ec7",
   "metadata": {},
   "source": [
    "## e) No gradient registered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07edb3f1",
   "metadata": {},
   "source": [
    "Some `tf.Operations` (https://www.tensorflow.org/api_docs/python/tf/Operation) are registered as being non-differentiable and will return None. Others have no gradient registered.\n",
    "\n",
    "The `tf.raw_ops` (https://www.tensorflow.org/api_docs/python/tf/raw_ops) page shows which low-level ops have gradients registered.\n",
    "\n",
    "If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning None. This way you know something has gone wrong.\n",
    "\n",
    "For example, the tf.image.adjust_contrast function wraps raw_ops.AdjustContrastv2, which could have a gradient but the gradient is not implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d74af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
    "delta = tf.Variable(0.1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  new_image = tf.image.adjust_contrast(image, delta)\n",
    "\n",
    "try:\n",
    "  print(tape.gradient(new_image, [image, delta]))\n",
    "  assert False   # This should not happen.\n",
    "except LookupError as e:\n",
    "  print(f'{type(e).__name__}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb076d7",
   "metadata": {},
   "source": [
    "If you need to differentiate through this op, you'll either need to implement the gradient and register it (using `tf.RegisterGradient` https://www.tensorflow.org/api_docs/python/tf/RegisterGradient) or re-implement the function using other ops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc148e9",
   "metadata": {},
   "source": [
    "### Zeros instead of None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a598d9",
   "metadata": {},
   "source": [
    "In some cases it would be convenient to get 0 instead of None for unconnected gradients. You can decide what to return when you have unconnected gradients using the unconnected_gradients argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "decdd758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff56bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bb220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7615447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a4cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784eb20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5dbff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "564a2804",
   "metadata": {},
   "source": [
    "### https://www.tensorflow.org/api_docs/python/tf/GradientTape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4558b",
   "metadata": {},
   "source": [
    "### BE CAREFUL!\n",
    "Note that when using models you should ensure that your variables exist when using watch_accessed_variables=False. Otherwise it's quite easy to make your first iteration not have any gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.keras.layers.Dense(32)\n",
    "b = tf.keras.layers.Dense(32)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n",
    "                           # `a.variables` will return an empty list and the\n",
    "                           # tape will not be watching anything.\n",
    "  result = b(a(inputs))\n",
    "  tape.gradient(result, a.variables)  # The result of this computation will be\n",
    "                                      # a list of `None`s since a's variables\n",
    "                                      # are not being watched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50caad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0077f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b063e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f218b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
